\documentclass[fleqn]{hw}

\usepackage{float}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\title{HW3: Markov Decision Processes}
\duedate{}
\class{CS6300: Artificial Intelligence, Spring 2018}
\institute{University of Utah}
\author{Jake Pitkin}
% IF YOU'RE USING THIS .TEX FILE AS A TEMPLATE, PLEASE REPLACE
% The author WITH YOUR NAME AND UID.
% Replace the due date with anyone you worked with i.e. "Worked with: John McCarthy, Watson, & Hal-9000"
\begin{document}
\maketitle

% IF YOU'RE USING THIS .TEX FILE AS A TEMPLATE, PLEASE REPLACE
% "CS5300, Spring 2009" WITH YOUR NAME AND UID.

% Hand in at: http://www.cs.utah.edu/~hal/handin.pl?course=cs5300

\section{Life as a Professor}

\begin{enumerate}

\item Using value iteration we can calculate $V_t^*(state)$ for $t = 1\dots5$ with $\gamma = 0.5$. We will use the following rewards for each state and the recursive value iteration formula:

$$r_{asst} = 20, \quad r_{assc} = 60, \quad r_{full} = 400, \quad r_{hl} = 10, \quad r_{dead} = 0$$
$$V_s^1 = r_s, \quad V_s^t = r_s + \gamma \sum_{j=1}^n p_{sj} V_j^{t-1}$$

\begin{table}[H]
\centering	
\begin{tabular}{|c||c|c|c|c|c|}
\hline
{\bf t} & $V_t^*(\text{asst})$ & $V_t^*(\text{assc})$ & $V_t^*(\text{full})$ & $V_t^*(\text{hl})$ & $V_t^*(\text{dead})$ \\
\hline
$0$ & $0$ & $0$ & $0$ & $0$ & $0$ \\
\hline
$1$ & $20$ & $60$ & $400$ & $10$ & $0$ \\
\hline
$2$ & $33$ & $119$ & $540$ & $13.5$ & $0$ \\
\hline
$3$ & $43.15$ & $151.05$ & $589$ & $14.725$ & $0$ \\
\hline
$4$ & $49.5225$ & $165.6875$ & $606.15$ & $15.15375$ & $0$ \\
\hline
$5$ & $52.940875$ & $171.836625$ & $612.1525$ & $15.3038125$ & $0$ \\
\hline
\end{tabular}
\caption{Five iterations of value iteration on the professor MDP.}
\end{table}

\item To compute the max-norm of $V$ between steps 4 and 5, we will consider the change for each state and take the maximum:

$$||U|| = max(V_5(asst) - V_4(asst), V_5(assc) - V_4(assc), V_5(full) - V_4(full), V_5(hl) - V_4(hl), V_5(dead) - V_4(dead))$$
$$||U|| = max(3.418375, 6.151325, 6.0025, 0.1500625, 0)$$
$$||U|| = 6.151325$$

So the final values are each at most $6.151325$ away from the true values (as the Bellman update is a contraction so the max-norm will be strictly decreasing with additional iterations of value iteration).

From the text (RN pages 654-655), we know that the Bellman update is a contraction by a factor of $\gamma$ on the space of utility vectors. Let $B$ be the Bellman update, $U_i$ be the utility vector and $U_i^\prime$ be the utility vector at the next step. We can say:

$$||BU_i - B_i^\prime|| \leq \gamma || U_i - U_i^\prime||$$

Simply put, the Bellman update when applied to $U_i$ and $U_i^\prime$ will produce values that are closer together by a factor of $\gamma$. Since $\gamma = 0.5$, the next update will produce a value at most roughly $3$ away from the true value (half of $6.151$ the max-norm). 

So if we are ok with a small error, say being within 1 of the true value, we can expect to be at most three more steps away from convergence.

\end{enumerate}

\newpage

\section{Clarence the Evil Professor}

\begin{enumerate}

\item Alice's policy is to always work. This means we can remove the $max$ from the value iteration formula with actions added and just consider the action \textit{work}:

$$r_{DF} = 0, \quad r_{SF} = 0, \quad r_{DP} = 10, \quad r_{SP} = 10$$
$$V_s^1 = r_s, \quad V_s^t = r_s + \gamma \sum_{j = 1}^n p_{sj}^{work} V_j^{t-1}$$

We will consider this for $t = 0\dots4$ and $\gamma = 0.5$:

\begin{table}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
\hline
{\bf t} & D+F & S+F & D+P & S+P \\
\hline
$0$ & $0$ & $0$ & $0$  & $0$  \\
\hline
$1$ & $0$ & $0$ & $10$ & $10$ \\
\hline
$2$ & $0$ & $0$ & $10$ & $10$ \\
\hline
$3$ & $0$ & $0$ & $10$ & $10$ \\
\hline
$4$ & $0$ & $0$ & $10$ & $10$ \\
\hline
\end{tabular}
\caption{Policy: always work.}
\end{table}

This outcome makes sense. Professor Clarence doesn't value hard work and \textit{work} only leads to \textit{Dumb \& Fail} or \textit{Smart \& Fail}. As such, the most value Alice can obtain is 10 by beginning in the state \textit{Dumb \& Pass} or \textit{Smart \& Pass}.

\item Alice now updates her policy to work when she's dumb and sweet talk when she's smart. Using the same formula as part 1 with this updated policy:

\begin{table}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
\hline
{\bf t} & D+F & S+F & D+P & S+P \\
\hline
$0$ & $0$ & $0$ & $0$  & $0$  \\
\hline
$1$ & $0$ & $0$ & $10$ & $10$ \\
\hline
$2$ & $0$ & $2.5$ & $10$ & $15$ \\
\hline
$3$ & $0.625$ & $3.75$ & $10.625$ & $16.25$ \\
\hline
$4$ & $1.09375$ & $4.21875$ & $11.09375$ & $16.71875$ \\
\hline
\end{tabular}
\caption{Policy: work when dumb and sweet talk when smart.}
\end{table}

This policy is better which makes sense as Alice incorporates sweet talking into her policy. Professor Clarence only passes students that sweet talk him. This allows students in any state to have a chance to move into a passing state.

\newpage
\item Now we will run value iteration without a policy. Calculating the value of $V$ for $t = 1,\dots,5$ and $\gamma = 0.5$ using the value iteration formula for multiple actions:

$$r_{DF} = 0, \quad r_{SF} = 0, \quad r_{DP} = 10, \quad r_{SP} = 10$$
$$V_s^1 = r_s, \quad V_s^t = \max_l \Big[ r_s + \gamma \sum_{j = 1}^n p_{sj}^l V_j^{t-1}\Big]$$

\begin{table}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
\hline
{\bf t} & D+F & S+F & D+P & S+P \\
\hline
$0$ & $0$ & $0$ & $0$  & $0$  \\
\hline
$1$ & $0$ & $0$ & $10$ & $10$ \\
\hline
$2$ & $0$ & $2.5$ & $12.5$ & $15$ \\
\hline
$3$ & $0.625$ & $3.75$ & $13.125$ & $16.875$ \\
\hline
$4$ & $1.09375$ & $4.375$ & $13.4375$ & $17.5$ \\
\hline
\end{tabular}
\caption{Value iteration without an initial policy.}
\end{table}

\item For a fixed policy $\pi$, which was to always work, values were found for each state:
$$V^{\pi_i}(D + F) = 0, \quad V^{\pi_i}(S + F) = 0, \quad V^{\pi_i}(D + P) = 10, \quad V^{\pi_i}(S + P) = 10$$

Using these values and the MDP graph, we will use policy extraction to estimate a new policy for each state:

$$\pi_{i+1}(s) = \max_a \Big[R(s, a) + \gamma \sum_{s^\prime} T(s^\prime | s, a)V^{\pi_i}(s^\prime)\Big]$$

\begin{table}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
\hline
 & D+F & S+F & D+P & S+P \\
\hline
{\it sweet talk} & $0$ & $2.5$  & $12.5$  & $15$ \\ \hline
{\it work} & $0$ & $0$ & $10$ & $10$ \\ \hline
$\pi_{i+1}(s)$ & {\it work} & {\it sweet talk} & {\it sweet talk} & {\it sweet talk} \\
\hline
\end{tabular}
\caption{Estimated policy using the values from part 1.}
\end{table}

The new policy for each state is to {\it work} if you are {\it Dumb \& Failing} and {\it sweet talk} otherwise (with an arbitrary choice made for the tie for D + F). We will perform value iteration on this policy with $t = 1,\dots,5$ and $\gamma = 0.5$:

\begin{table}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
\hline
{\bf t} & D+F & S+F & D+P & S+P \\
\hline
$1$ & $0$ & $0$ & $10$ & $10$ \\
\hline
$2$ & $0$ & $2.5$ & $12.5$ & $15$ \\
\hline
$3$ & $0.625$ & $3.75$ & $13.125$ & $16.875$ \\
\hline
$4$ & $1.09375$ & $4.375$ & $13.4375$ & $17.5$ \\
\hline
$5$ & $1.3671875$ & $4.6484375$ & $13.6328125$ & $17.734375$ \\
\hline
\end{tabular}
\caption{Value iteration with a fixed policy of sweet talking.}
\end{table}

Using these values, we will perform another round of policy extraction for a final policy:

\begin{table}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
\hline
 & D+F & S+F & D+P & S+P \\
\hline
{\it sweet talk} & $0$ & $4.375$  & $13.3203125$  & $17.6953125$ \\ \hline
{\it work} & $1.0546875$ & $2.109375$ & $11.0546875$ & $12.109375$ \\ \hline
$\pi_{i+1}(s)$ & {\it work} & {\it sweet talk} & {\it sweet talk} & {\it sweet talk} \\
\hline
\end{tabular}
\caption{Third policy.}
\end{table}

The policy has converged as it hasn't changed since the last step of policy extraction. 

This policy seems intuitively optimal to me. When in the state D+F, the only way to leave is to {\it work}. In the other states, {\it sweet talking} has a better chance of putting you in a passing state and {\it working} has a better of putting you in a failing state.

\end{enumerate}

\end{document}
