{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "### Jake Pitkin\n",
    "**CS 6190: Probabilistic Modeling - Spring 2018**<br>\n",
    "**February 1st 2018**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1$. **Expectation of sufficient statistics:** Consider a random variable $X$ from a continuous exponential family with _natural_ parameter $\\eta = (\\eta_1,...,\\eta_n)$. Recall that this means the pdf is of the form:\n",
    "\n",
    "$$p(x) = h(x) \\ exp(\\eta \\cdot T(x) - A(\\eta))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1a) Show that $E[T(X) \\ | \\ \\eta] = \\nabla \\ A(\\eta) = (\\frac{dA}{d\\eta_1},...,\\frac{dA}{d\\eta_d})$.\n",
    "\n",
    "Using the hint, we will start by applying the identity $\\int p(x) \\ dx = 1$:\n",
    "\n",
    "$$\\int h(x) \\ exp(\\eta \\cdot T(x) - A(\\eta)) \\ dx = 1$$\n",
    "\n",
    "Then we will take the derivative with respect to $\\eta$ on both sides using the chain rule:\n",
    "\n",
    "$$\\frac{d}{d\\eta} \\int h(x) \\ exp(\\eta \\cdot T(x) - A(\\eta)) \\ dx = 0$$\n",
    "\n",
    "$$\\int h(x) \\ exp(\\eta \\cdot T(x) - A(\\eta)) \\ \\eta^\\prime \\ T(x) - A^\\prime(\\eta) \\ dx = 0$$\n",
    "\n",
    "Substituting back in the definition of the pdf $p(x)$:\n",
    "\n",
    "$$\\int p(x) \\ \\eta^\\prime \\ T(x) - A^\\prime(\\eta) \\ dx = 0$$\n",
    "\n",
    "Taking the definition of expectation of $T(x)$:\n",
    "\n",
    "$$E[T(x)] \\ \\eta^\\prime - A^\\prime(\\eta) = 0$$\n",
    "\n",
    "Re-arranging terms:\n",
    "\n",
    "$$E[T(x)] = \\frac{A^\\prime(\\eta)}{\\eta^\\prime} = \\frac{dA}{d\\eta}$$\n",
    "\n",
    "Thus given a _natural_ parameter $\\eta = (\\eta_1,...,\\eta_n)$, we have shown the expecation of $T(X)$ is:\n",
    "\n",
    "$$E[T(X)\\ | \\ \\eta] = (\\frac{dA}{d\\eta_1},...,\\frac{dA}{d\\eta_d}) = \\nabla \\ A(\\eta)$$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(1b) Verify this formula works for the Gaussian distribution with unknown mean, $\\mu$, and known variance, $\\sigma^2$.\n",
    "\n",
    "From [wikipedia](https://en.wikipedia.org/wiki/Exponential_family) we have the following for the Gaussian distribution with unknown mean and known variance:\n",
    "\n",
    "$$A(\\eta) = \\frac{\\eta^2}{2} \\qquad \\eta = \\frac{\\mu}{\\sigma} \\qquad T(x) = \\frac{x}{\\sigma}$$\n",
    "\n",
    "First will take the derivative of $A(\\eta)$ with respect to $\\eta$:\n",
    "\n",
    "$$\\frac{dA}{d\\eta}\\frac{\\eta^2}{2} = \\eta = \\frac{\\mu}{\\sigma}$$\n",
    "\n",
    "Then we will take the expectation of $T(x)$ for the Gaussian:\n",
    "\n",
    "$$E[T(x)] = \\int T(x) \\ p(x) \\ dx$$\n",
    "\n",
    "$$E[T(x)] = \\int \\frac{x}{\\sigma} \\ p(x) \\ dx$$\n",
    "\n",
    "$$E[T(x)] = \\frac{1}{\\sigma} \\int x \\ p(x) \\ dx$$\n",
    "\n",
    "We know the expecation of the Gaussian distrobution is the mean $\\mu$ from [wikipedia](https://en.wikipedia.org/wiki/Normal_distribution):\n",
    "\n",
    "$$E[T(x)] = \\frac{\\mu}{\\sigma}$$\n",
    "\n",
    "Thus we have shown $E[T(x)] = \\frac{\\mu}{\\sigma} = A(\\eta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2$. **Noninformative priors for the Poisson distribution:** Let $X \\sim Pois(\\lambda)$. Recall the pmf for the Poisson is\n",
    "\n",
    "$$P(X = k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2a) Rewrite the above pmf in exponential family form. What is the natural parameter? What is the sufficient statistic?\n",
    "\n",
    "We will start by using the trick seen in class of taking the $log$ followed by the $exponential$ of both sides:\n",
    "\n",
    "$$p(x) = \\frac{\\lambda^k e^{-\\lambda}}{x!}$$\n",
    "\n",
    "$$log(p(x)) = log(\\frac{\\lambda^k e^{-\\lambda}}{x!})$$\n",
    "\n",
    "$$log(p(x)) = log(\\lambda^x) + log(e^{-\\lambda}) - log(x!)$$\n",
    "\n",
    "$$log p(x) = x log(\\lambda) - \\lambda - log(x!)$$\n",
    "\n",
    "Taking the $exponential$ of both sides:\n",
    "\n",
    "$$p(x) = exp(x log(\\lambda) - \\lambda - log(x!))$$\n",
    "\n",
    "$$p(x) = \\frac{1}{x!} exp(log(\\lambda) \\cdot x - \\lambda)$$\n",
    "\n",
    "With the pmf in a similar form to the exponental family, we can see the components:\n",
    "\n",
    "$$h(x) = \\frac{1}{x!} \\qquad \\eta = log(\\lambda) \\qquad T(x) = x \\qquad A(\\eta) = e^\\lambda$$\n",
    "\n",
    "With the natural parameter being $log(\\lambda)$ and the sufficient statistic being $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2b) Give at least two different options for noninformative priors for $p(\\lambda)$.\n",
    "\n",
    "Two options for noninformative priors for $p(\\lambda)$ are Jeffrey's Prior and the Gamma distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2c) What are the resulting posteriors for your two options? Are they proper (i.e., can they be normalized?)\n",
    "\n",
    "**Jeffrey's Prior:**\n",
    "\n",
    "First we will calculate the Jeffrey's Prior for the Poisson distribution which is given by:\n",
    "\n",
    "$$p(\\lambda) = \\sqrt{I(\\lambda)} = \\sqrt{- E[\\frac{d^2}{d\\lambda^2} \\ log \\ p(x \\ | \\ \\lambda) \\ | \\lambda]}$$\n",
    "\n",
    "We will solve for the second derivative of the log-likelihood function first:\n",
    "\n",
    "$$p(x \\ | \\ \\lambda) = \\prod_{i = 1}^n \\frac{\\lambda^{x_i} \\ exp(-\\lambda)}{x_i!}$$\n",
    "\n",
    "$$log(p(x \\ | \\ \\lambda)) = \\sum_{i = 1}^n x_i log(\\lambda) - n\\lambda - \\sum_{i = 1}^n log(x_i!)$$\n",
    "\n",
    "Taking the first derivative with respect to $\\lambda$:\n",
    "\n",
    "$$\\frac{d \\ log(p(x \\ | \\ \\lambda))}{d \\lambda} = \\frac{\\sum_{i = 1}^n x_i}{\\lambda} - n$$\n",
    "\n",
    "Taking the second derivative with respect to $\\lambda$:\n",
    "\n",
    "$$\\frac{d^2 \\ log(p(x \\ | \\ \\lambda))}{d^2 \\lambda} = -\\frac{\\sum_{i = 1}^n x_i}{\\lambda^2}$$\n",
    "\n",
    "Next we take the expectation (recall $\\lambda$ is the mean):\n",
    "\n",
    "$$E\\Big[-\\frac{\\sum_{i = 1}^n x_i}{\\lambda^2} \\ | \\ \\lambda \\Big] = \\frac{-n\\lambda}{\\lambda^2} = \\frac{-n}{\\lambda}$$\n",
    "\n",
    "With this we can calculate Jeffrey's Prior for the Poisson distribution:\n",
    "\n",
    "$$p(\\lambda) = \\sqrt{-(\\frac{-n}{\\lambda})} \\propto \\frac{1}{\\sqrt{\\lambda}}$$\n",
    "\n",
    "Finally, we have the likelihood and prior so we can calculate the posterior:\n",
    "\n",
    "$$p(\\lambda \\ | \\ x) = p(x \\ | \\ \\lambda) * p(\\lambda)$$\n",
    "\n",
    "$$p(\\lambda \\ | \\ x) = \\prod_{i = 1}^n \\frac{\\lambda^{x_i} \\ exp(-\\lambda)}{x_i!} * \\frac{1}{\\sqrt{\\lambda}}$$\n",
    "\n",
    "I don't think this is proper as $\\frac{1}{\\sqrt{\\lambda}}$ will be undefined in the integral that forms the normalizing constant.\n",
    "\n",
    "**Gamma Distribution**\n",
    "\n",
    "From the [wikipedia](https://en.wikipedia.org/wiki/Poisson_distribution) for the Poisson distribution, we know that the Gamma distribution is a conjugate prior. The Gamma distribution as a prior is given as:\n",
    "\n",
    "$$p(\\lambda; \\alpha, \\beta) = \\frac{\\beta^\\alpha  \\ \\lambda^{\\alpha-1} \\ exp(-\\beta \\lambda)}{\\Gamma(\\lambda)}$$ \n",
    "\n",
    "Combining this with the Poisson likelihood to get the posterior pdf:\n",
    "\n",
    "$$p(\\lambda \\ | \\ x) = \\frac{\\lambda^{x} \\ exp(-\\lambda)}{x!} * p(\\lambda; \\alpha, \\beta)$$\n",
    "\n",
    "$$p(\\lambda \\ | \\ x) = \\frac{\\lambda^{x} \\ exp(-\\lambda)}{x!} \\frac{\\beta^\\alpha  \\ \\lambda^{\\alpha-1} \\ exp(-\\beta \\lambda)}{\\Gamma(\\lambda)}$$\n",
    "\n",
    "Removing terms that are constant with respect to $\\lambda$:\n",
    "\n",
    "$$p(\\lambda \\ | \\ x) = \\frac{\\lambda^{x} \\ exp(-\\lambda)}{x!} \\frac{\\beta^\\alpha  \\ \\lambda^{\\alpha-1} \\ exp(-\\beta \\lambda)}{\\Gamma(\\lambda)} \\propto \\lambda^x \\ exp(-\\lambda) \\ \\lambda^{\\alpha-1} \\ exp(-\\beta \\lambda)$$\n",
    "\n",
    "Combining terms:\n",
    "\n",
    "$$p(\\lambda \\ | \\ x) = \\lambda^{x + \\alpha -1} \\ exp(-\\lambda-\\beta \\lambda)$$\n",
    "\n",
    "Which is equivalent to:\n",
    "\n",
    "$$Gamma(\\lambda, (\\alpha + x), (\\beta + 1))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3$. **Non-conjugate priors:** Let $X_i$ be from a Gaussian with known variance $\\sigma^2$ and mean $\\mu$ with _uniform prior_, i.e.,\n",
    "\n",
    "$$\\mu \\sim Unif(a, b)$$\n",
    "$$X_i \\sim N(\\mu, \\sigma^2)$$\n",
    "\n",
    "What is the posterior pdf, $p(\\mu \\ | \\ x_1,...,x_n;\\sigma^2, a, b$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Coding Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4$. Use a conjuate prior $(\\mu_j, \\sigma_j^2) \\sim N-IG(0, 10^{-6}, 10^{-6}, 10^{-6})$, independently for each group $j$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
