{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: MCMC\n",
    "### Jake Pitkin\n",
    "**CS 6190: Probabilistic Modeling - Spring 2018**<br>\n",
    "**April 7 2018**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1.$ Write a function to perform Gibbs sampling of a binary label image $x$ with the Ising model prior and iid Gaussian likelihood, given a noisy image $y$. This function should take $\\alpha$, $\\beta$, and $\\sigma$ parameters, and generate a random binary image (labels in the set $\\{-1,1\\}$ according to the posterior Gibbs distribution for $x|y.$ The energy should look like this:\n",
    "\n",
    "$$U(x) = -\\alpha \\sum_{i} x_i - \\beta \\sum_{\\langle i,j\\rangle} x_ix_j + \\frac{1}{2\\sigma^2} \\sum_{i} (x_i - y_i)^2$$\n",
    "\n",
    "Note this is assuming that the $x_i$ labels are also the mean pixel values in the Gaussian. The $\\alpha$ parameter controls the proportion of labels that are $-1$ versus $+1$. Negative values of $\\alpha$ will favor more $-1$ pixels, and positive values of $\\alpha$ will favor more $+1$ pixels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "#IMAGE_WIDTH = None\n",
    "#IMAGE_HEIGHT = None\n",
    "#Y = None\n",
    "#X = None\n",
    "\n",
    "def read_image(path):\n",
    "    img = Image.open(path, 'r')\n",
    "    pixels = img.load()\n",
    "    width = img.size[0]\n",
    "    height = img.size[1]\n",
    "    Y = [[(pixels[x,y]/255) * 20 - 10 for x in range(IMAGE_WIDTH)] for y in range(IMAGE_HEIGHT)]\n",
    "    #X = [[0 for x in range(IMAGE_WIDTH)] for y in range(IMAGE_HEIGHT)]\n",
    "    return Y, width, height\n",
    "    \n",
    "def get_neighbours(X, row, col, width, height):\n",
    "    # left, right, up, and down initialized to wrap values\n",
    "    neighbours = [X[row][width-1], X[row][0], X[height-1][col], X[0][col]]\n",
    "    # check if we don't have to wrap and replace\n",
    "    if col != 0: # left\n",
    "        neighbours[0] = X[row][col-1]\n",
    "    if col != width-1: #right\n",
    "        neighbours[1] = X[row][col+1]\n",
    "    if row != 0: #up\n",
    "        neighbours[2] = X[row-1][col]\n",
    "    if row != height-1:\n",
    "        neighbours[3] = X[row+1][col]\n",
    "    return neighbours\n",
    "\n",
    "def random_init_matrix(width, height):\n",
    "    return [[random.choice([-1, 1]) for x in range(width)] for y in range(height)]\n",
    "\n",
    "def ising_prior_norm(alpha, beta, X, row, col, width, height):\n",
    "    prob = alpha * X[row][col]\n",
    "    for n in get_neighbours(X, row, col, width, height):\n",
    "        prob += beta * X[row][col] * n\n",
    "    prob = math.exp(prob)\n",
    "    Z = \n",
    "    return math.exp(prob)\n",
    "\n",
    "def energy(alpha, beta, sigma, row, col):\n",
    "    neighbours = get_neighbours(row, col)\n",
    "    energy = -1 * alpha * X[row][col]\n",
    "    for n in neighbours:\n",
    "        energy -= beta * X[row][col] * n\n",
    "    energy += (1/(2*math.pow(sigma, 2))) * math.pow(X[row][col]-Y[row][col], 2)\n",
    "    return energy\n",
    "\n",
    "def energy_norm(alpha, beta, sigma, row, col, x):\n",
    "    neighbours = get_neighbours(row, col)\n",
    "    energy = -1 * alpha * x\n",
    "    for n in neighbours:\n",
    "        energy -= beta * x * n\n",
    "    energy += (1/(2*math.pow(sigma, 2))) * math.pow(x-Y[row][col], 2)\n",
    "    return energy\n",
    "\n",
    "def cond_dist_single_pixel(alpha, beta, sigma, row, col):\n",
    "    cond_dist = math.exp(-1 * energy(alpha, beta, sigma, row, col))\n",
    "    print(cond_dist)\n",
    "    normalize = math.exp(-1 * energy_norm(alpha, beta, sigma, row, col, 1))\n",
    "    print(normalize)\n",
    "    normalize += math.exp(-1 * energy_norm(alpha, beta, sigma, row, col, -1))\n",
    "    print(normalize)\n",
    "    #return cond_dist / normalize\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Run your code with just the Ising prior term (no posterior). Do this a few times and generate several random binary images. Try different $\\alpha$ and $\\beta$ terms to see what the effects are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "alpha = 5\n",
    "beta = 0\n",
    "k = 100\n",
    "width = 100\n",
    "height = 100\n",
    "X = random_init_matrix(width, height)\n",
    "\n",
    "def ising_prior(alpha, beta, x, X, row, col, width, height):\n",
    "    prob = -1 * alpha * x\n",
    "    for n in get_neighbours(X, row, col, width, height):\n",
    "        prob += -1 * beta * x * n\n",
    "    return prob\n",
    "\n",
    "def ising_normalized(alpha, beta, X, row, col, width, height):\n",
    "    prob = math.exp(-1 * ising_prior(alpha, beta, X[row][col], X, row, col, width, height))\n",
    "    normalize = math.exp(-1 * ising_prior(alpha, beta, 1, X, row, col, width, height))\n",
    "    normalize += math.exp(-1 * ising_prior(alpha, beta, -1, X, row, col, width, height))\n",
    "    return prob / normalize\n",
    "\n",
    "def pixels_to_image(path, X, width, height):\n",
    "    cmap = { -1 : (0, 0, 0), 1 : (255, 255, 255)}\n",
    "    data = []\n",
    "    for row in range(width):\n",
    "        for col in range(height):\n",
    "            data.append(cmap[X[row][col]])\n",
    "    img = Image.new('RGB', (width, height), \"blue\")\n",
    "    img.putdata(data)\n",
    "    img.show()\n",
    "\n",
    "for _ in range(k):\n",
    "    for row in range(width):\n",
    "        for col in range(height):\n",
    "            if (row + col) % 2 == 0:\n",
    "                continue\n",
    "            if random.uniform(0, 1) < ising_normalized(alpha, beta, X, row, col, width, height):\n",
    "                X[row][col] = 1\n",
    "            else:\n",
    "                X[row][col] = -1\n",
    "    for row in range(100):\n",
    "        for col in range(100):\n",
    "            if (row + col) % 2 == 1:\n",
    "                continue\n",
    "            if random.uniform(0, 1) > ising_normalized(alpha, beta, X, row, col, width, height):\n",
    "                X[row][col] = X[row][col] * -1\n",
    "    pixels_to_image(\"test.png\", X, width, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Run your code on the images ${\\tt noisy}$-${\\tt message.png}$ and ${\\tt noisy}$-${\\tt yinyang.ong}$. After you read an image, you need to apply the following intensity transform to the pixels (to get black $= -1$ and white $= +1$): \n",
    "\n",
    "$x \\ * \\ 20 \\ - \\ 10$\n",
    "\n",
    "Compute the posterior mean images for both examples (again, don't forget to burn-in). You can fix values for $\\alpha$, $\\beta$, and $\\sigma^2$ (you will want to tune these manually to get something that works well). What values of $\\alpha, \\beta, \\sigma^2$ did you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Use your posterior samples to iteratively estimate $\\sigma^2$ from the data. That is, assume the \"clean\" image that you sample is the true image, and use it to get an MLE of $\\sigma^2$ (update this estimate each iteration). What final estimate do you get for $\\sigma^2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2.$ Say you are given data $(X,Y)$, with $X \\in \\mathbb{R}^d$ and $Y \\in \\{0,1\\}$. The goal is to train a classifier that will predict an unknown class label $\\tilde{y}$ from a new data point $\\tilde{x}$. Consider the following model:\n",
    "\n",
    "$$Y \\sim Ber\\Big(\\frac{1}{1+e^{-X^T\\beta}}\\Big),$$\n",
    "$$\\beta \\sim N(0, \\sigma^2I).$$\n",
    "\n",
    "This is a Bayesian logistic regression model. Your goal is to derive and implement a Hamiltonian Monte Carlo sampler for doing Bayesian inference on $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Write down the formula for the unormalized posterior of $\\beta|Y$, i.e.,\n",
    "\n",
    "$$p(\\beta|y;x,\\sigma) \\propto \\prod_{i=1}^n p(y_i|\\beta;x_i)p(\\beta;\\sigma)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Show that this posterior is proportional to exp($-U(\\beta))$), where\n",
    "\n",
    "$$U(\\beta) = \\sum_{i=1}^n (1 - y_i)x_i^T \\beta + log(1 + e^{-x_i^T \\beta}) + \\frac{1}{2\\sigma^2}||\\beta||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Implement a Hamiltonian Monte Carlo Routine in Python for sampling from the posterior of $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Use your code to analyze the ${\\tt iris}$ data in Python, looking only at two species, *versicolor* and *virginica*. The species labels are your $Y$ data, and the four features, petal length and width, sepal length and width, are your $X$ data. Also, add a constant term, i.e., a columns of 1's to your $X$ matrix. Use the first 30 rows for each species as training data and leave out the last $20$ rows for each species as test data (for a total of $60$ training and $40$ testing). Generate samples of $\\beta$ (don't forget to burn-in), and use these to get a prediction, $\\tilde{y}$, of the class labels for the test data. Use your samples to get a Monte Carlo estimate of the posterior predictive probability $p(\\tilde{y}|y)$ for each testing data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** Draw trace plots of your $\\beta$ sequence and histograms (do 1D plots of each four vector components separately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)** Compare this to the true class labels, $y$, and see how well you did by estimating the average error rate, $E[|y - \\tilde{y}|]$ (a.k.a. the zero-one loss). What values of $\\sigma$, $\\epsilon$, and $L$ did you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
