{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: MCMC\n",
    "### Jake Pitkin\n",
    "**CS 6190: Probabilistic Modeling - Spring 2018**<br>\n",
    "**April 7 2018**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1.$ Write a function to perform Gibbs sampling of a binary label image $x$ with the Ising model prior and iid Gaussian likelihood, given a noisy image $y$. This function should take $\\alpha$, $\\beta$, and $\\sigma$ parameters, and generate a random binary image (labels in the set $\\{-1,1\\}$ according to the posterior Gibbs distribution for $x|y.$ The energy should look like this:\n",
    "\n",
    "$$U(x) = -\\alpha \\sum_{i} x_i - \\beta \\sum_{\\langle i,j\\rangle} x_ix_j + \\frac{1}{2\\sigma^2} \\sum_{i} (x_i - y_i)^2$$\n",
    "\n",
    "Note this is assuming that the $x_i$ labels are also the mean pixel values in the Gaussian. The $\\alpha$ parameter controls the proportion of labels that are $-1$ versus $+1$. Negative values of $\\alpha$ will favor more $-1$ pixels, and positive values of $\\alpha$ will favor more $+1$ pixels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Run your code with just the Ising prior term (no posterior). Do this a few times and generate several random binary images. Try different $\\alpha$ and $\\beta$ terms to see what the effects are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Run your code on the images ${\\tt noisy}$-${\\tt message.png}$ and ${\\tt noisy}$-${\\tt yinyang.ong}$. After you read an image, you need to apply the following intensity transform to the pixels (to get black $= -1$ and white $= +1$): \n",
    "\n",
    "$x \\ * \\ 20 \\ - \\ 10$\n",
    "\n",
    "Compute the posterior mean images for both examples (again, don't forget to burn-in). You can fix values for $\\alpha$, $\\beta$, and $\\sigma^2$ (you will want to tune these manually to get something that works well). What values of $\\alpha, \\beta, \\sigma^2$ did you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Use your posterior samples to iteratively estimate $\\sigma^2$ from the data. That is, assume the \"clean\" image that you sample is the true image, and use it to get an MLE of $\\sigma^2$ (update this estimate each iteration). What final estimate do you get for $\\sigma^2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2.$ Say you are given data $(X,Y)$, with $X \\in \\mathbb{R}^d$ and $Y \\in \\{0,1\\}$. The goal is to train a classifier that will predict an unknown class label $\\tilde{y}$ from a new data point $\\tilde{x}$. Consider the following model:\n",
    "\n",
    "$$Y \\sim Ber\\Big(\\frac{1}{1+e^{-X^T\\beta}}\\Big),$$\n",
    "$$\\beta \\sim N(0, \\sigma^2I).$$\n",
    "\n",
    "This is a Bayesian logistic regression model. Your goal is to derive and implement a Hamiltonian Monte Carlo sampler for doing Bayesian inference on $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Write down the formula for the unormalized posterior of $\\beta|Y$, i.e.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Show that this posterior is proportional to exp($-U(\\beta))$), where\n",
    "\n",
    "$$U(\\beta) = \\sum_{i=1}^n (1 - y_i)x_i^T \\beta + log(1 + e^{-x_i^T \\beta}) + \\frac{1}{2\\sigma^2}||\\beta||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Implement a Hamiltonian Monte Carlo Routine in Python for sampling from the posterior of $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Use your code to analyze the ${\\tt iris}$ data in Python, looking only at two species, *versicolor* and *virginica*. The species labels are your $Y$ data, and the four features, petal length and width, sepal length and width, are your $X$ data. Also, add a constant term, i.e., a columns of 1's to your $X$ matrix. Use the first 30 rows for each species as training data and leave out the last $20$ rows for each species as test data (for a total of $60$ training and $40$ testing). Generate samples of $\\beta$ (don't forget to burn-in), and use these to get a prediction, $\\tilde{y}$, of the class labels for the test data. Use your samples to get a Monte Carlo estimate of the posterior predictive probability $p(\\tilde{y}|y)$ for each testing data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** Draw trace plots of your $\\beta$ sequence and histograms (do 1D plots of each four vector components separately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)** Compare this to the true class labels, $y$, and see how well you did by estimating the average error rate, $E[|y - \\tilde{y}|]$ (a.k.a. the zero-one loss). What values of $\\sigma$, $\\epsilon$, and $L$ did you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
